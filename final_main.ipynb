{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: data/\n",
      "Created directory: models/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Ensure the directories for saving data exist\n",
    "def create_directories():\n",
    "    if not os.path.exists('data/'):\n",
    "        os.makedirs('data/')\n",
    "        print(\"Created directory: data/\")\n",
    "    if not os.path.exists('models/'):\n",
    "        os.makedirs('models/')\n",
    "        print(\"Created directory: models/\")\n",
    "\n",
    "create_directories()  # Create the necessary directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nduration: continuous.\\nprotocol_type: symbolic.\\nservice: symbolic.\\nflag: symbolic.\\nsrc_bytes: continuous.\\ndst_bytes: continuous.\\nland: symbolic.\\nwrong_fragment: continuous.\\nurgent: continuous.\\nhot: continuous.\\nnum_failed_logins: continuous.\\nlogged_in: symbolic.\\nnum_compromised: continuous.\\nroot_shell: continuous.\\nsu_attempted: continuous.\\nnum_root: continuous.\\nnum_file_creations: continuous.\\nnum_shells: continuous.\\nnum_access_files: continuous.\\nnum_outbound_cmds: continuous.\\nis_host_login: symbolic.\\nis_guest_login: symbolic.\\ncount: continuous.\\nsrv_count: continuous.\\nserror_rate: continuous.\\nsrv_serror_rate: continuous.\\nrerror_rate: continuous.\\nsrv_rerror_rate: continuous.\\nsame_srv_rate: continuous.\\ndiff_srv_rate: continuous.\\nsrv_diff_host_rate: continuous.\\ndst_host_count: continuous.\\ndst_host_srv_count: continuous.\\ndst_host_same_srv_rate: continuous.\\ndst_host_diff_srv_rate: continuous.\\ndst_host_same_src_port_rate: continuous.\\ndst_host_srv_diff_host_rate: continuous.\\ndst_host_serror_rate: continuous.\\ndst_host_srv_serror_rate: continuous.\\ndst_host_rerror_rate: continuous.\\ndst_host_srv_rerror_rate: continuous.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "duration: continuous.\n",
    "protocol_type: symbolic.\n",
    "service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_instance(instance, filename):\n",
    "    print(f\"Saving model instance: {filename}...\")\n",
    "    np.save(f'models/{filename}.npy', instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "def load_and_split_data(df, target_column='outcome', test_size=0.2, random_state=42):\n",
    "    print(\"Loading and splitting the dataset...\")\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    Y = df[target_column]  # Target\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state, stratify=Y)\n",
    "\n",
    "    # Save the split data\n",
    "    X_train.to_csv('data/X_train.csv', index=False)\n",
    "    X_test.to_csv('data/X_test.csv', index=False)\n",
    "    Y_train.to_csv('data/Y_train.csv', index=False)\n",
    "    Y_test.to_csv('data/Y_test.csv', index=False)\n",
    "\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "# Categorize the attack types into the 5 classes\n",
    "def categorize_attack_type(label):\n",
    "    if label in dos_attacks:\n",
    "        return 'DOS'\n",
    "    elif label in r2l_attacks:\n",
    "        return 'R2L'\n",
    "    elif label in u2r_attacks:\n",
    "        return 'U2R'\n",
    "    elif label in probe_attacks:\n",
    "        return 'probing'\n",
    "    else:\n",
    "        return 'normal'\n",
    "    \n",
    "\n",
    "# Example Usage\n",
    "df = pd.read_csv('data_with_column_names.csv')  # Replace with your actual dataset\n",
    "\n",
    "dos_attacks = ['smurf.', 'neptune.', 'back.', 'teardrop.', 'pod.', 'land.']\n",
    "r2l_attacks = ['warezclient.', 'guess_passwd.', 'imap.', 'warezmaster.', 'ftp_write.', 'phf.', 'spy.', 'multihop.']\n",
    "u2r_attacks = ['buffer_overflow.', 'loadmodule.', 'rootkit.', 'perl.']\n",
    "probe_attacks = ['satan.', 'ipsweep.', 'portsweep.', 'nmap.']\n",
    "df['outcome'] = df['outcome'].apply(categorize_attack_type)\n",
    "\n",
    "\n",
    "target_column = 'outcome'  # Define your target column\n",
    "# X_train, X_test, Y_train, Y_test = load_and_split_data(df, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical features\n",
    "def one_hot_encode(X_train, X_test, categorical_columns):\n",
    "    print(\"One-hot encoding categorical features...\")\n",
    "    encoders = {}\n",
    "    X_train_encoded_list = []\n",
    "    X_test_encoded_list = []\n",
    "    feature_names = []\n",
    "\n",
    "    # Loop through each categorical column and apply OneHotEncoder\n",
    "    for col in categorical_columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        # Fit and transform the training data\n",
    "        X_train_col_encoded = encoder.fit_transform(X_train[[col]])\n",
    "        X_train_encoded_list.append(X_train_col_encoded)\n",
    "        \n",
    "        # Transform the test data\n",
    "        X_test_col_encoded = encoder.transform(X_test[[col]])\n",
    "        X_test_encoded_list.append(X_test_col_encoded)\n",
    "\n",
    "        # Store the encoder for potential future use\n",
    "        encoders[col] = encoder\n",
    "        \n",
    "        # Get feature names and add to the feature names list\n",
    "        feature_names.extend(encoder.get_feature_names_out([col]))\n",
    "\n",
    "        # save the encoder as encoder_{col}.npy\n",
    "        save_model_instance(encoder, f'encoder_{col}')\n",
    "    \n",
    "    # Concatenate the encoded columns into a single matrix\n",
    "    X_train_encoded = np.concatenate(X_train_encoded_list, axis=1)\n",
    "    X_test_encoded = np.concatenate(X_test_encoded_list, axis=1)\n",
    "    \n",
    "    # Remove original categorical columns from X_train and X_test\n",
    "    X_train_remaining = X_train.drop(columns=categorical_columns)\n",
    "    X_test_remaining = X_test.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Concatenate the remaining columns with the one-hot encoded columns\n",
    "    X_train_final = np.concatenate([X_train_remaining.values, X_train_encoded], axis=1)\n",
    "    X_test_final = np.concatenate([X_test_remaining.values, X_test_encoded], axis=1)\n",
    "    \n",
    "    # Ensure the directory exists before saving the files\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "\n",
    "    # Save the encoded data\n",
    "    np.save('data/X_train_encoded.npy', X_train_final)\n",
    "    np.save('data/X_test_encoded.npy', X_test_final)\n",
    "\n",
    "    print(\"Categorical features encoded and original columns removed.\")\n",
    "    return X_train_final, X_test_final, encoders\n",
    "\n",
    "# Example usage with specified categorical columns\n",
    "categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "# X_train_encoded, X_test_encoded, encoders = one_hot_encode(X_train, X_test, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Robust Scaling\n",
    "def robust_scale(X_train, X_test):\n",
    "    print(\"Applying robust scaling...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Save the scaled data\n",
    "    np.save('data/X_train_scaled.npy', X_train_scaled)\n",
    "    np.save('data/X_test_scaled.npy', X_test_scaled)\n",
    "\n",
    "    print(\"Data scaled using RobustScaler.\")\n",
    "\n",
    "    save_model_instance(scaler, 'scaler')\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# X_train_scaled, X_test_scaled, scaler = robust_scale(X_train_encoded, X_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN  # Import SMOTEENN\n",
    "\n",
    "# Apply SMOTEENN resampling\n",
    "def apply_smoteenn(X_train, Y_train):\n",
    "    print(\"Applying SMOTEENN for resampling...\")\n",
    "    \n",
    "    # Initialize SMOTEENN with a suitable k_neighbors\n",
    "    smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "    \n",
    "    # Fit and resample the training data\n",
    "    X_train_resampled, Y_train_resampled = smote_enn.fit_resample(X_train, Y_train)\n",
    "\n",
    "    # Save resampled data\n",
    "    np.save('data/X_train_resampled.npy', X_train_resampled)\n",
    "    np.save('data/Y_train_resampled.npy', Y_train_resampled)\n",
    "\n",
    "    print(\"Data resampled using SMOTEENN.\")\n",
    "    print(f\"Resampled Y distribution: {Counter(Y_train_resampled)}\")\n",
    "\n",
    "    # dump resampled data into resampled_data.csv\n",
    "    resampled_data = pd.DataFrame(X_train_resampled)\n",
    "    resampled_data['outcome'] = Y_train_resampled\n",
    "    resampled_data.to_csv('resampled_data.csv', index=False)\n",
    "\n",
    "    \n",
    "    return X_train_resampled, Y_train_resampled\n",
    "\n",
    "# Assuming X_train_scaled and Y_train are defined and preprocessed\n",
    "# X_train_resampled, Y_train_resampled = apply_smoteenn(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def feature_selection(X_train, Y_train, variance_threshold=0.01, k_best=20, correlation_threshold=0.1):\n",
    "    print(\"Starting feature selection...\")\n",
    "\n",
    "    # Variance Threshold\n",
    "    vt = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_train_var = vt.fit_transform(X_train)\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = pd.DataFrame(X_train_var).corrwith(pd.Series(Y_train))\n",
    "    low_corr_features = correlation_matrix[correlation_matrix.abs() < correlation_threshold].index\n",
    "    X_train_var = np.delete(X_train_var, low_corr_features, axis=1)\n",
    "\n",
    "    print(f\"Removed low correlation features: {low_corr_features.tolist()}\")\n",
    "\n",
    "    # Keep one feature from highly correlated features\n",
    "    corr_matrix = pd.DataFrame(X_train_var).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "    # Retain only one feature from the highly correlated ones\n",
    "    X_train_var = np.delete(X_train_var, to_drop, axis=1)\n",
    "    print(f\"Removed highly correlated features: {to_drop}\")\n",
    "\n",
    "    # Mutual Information to select K best features\n",
    "    selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "    X_train_selected = selector.fit_transform(X_train_var, Y_train)\n",
    "\n",
    "    # Save the selected features\n",
    "    np.save('data/X_train_selected.npy', X_train_selected)\n",
    "\n",
    "    print(f\"Selected {k_best} best features using mutual information.\")\n",
    "    save_model_instance(selector, 'selector')\n",
    "    return X_train_selected, selector\n",
    "\n",
    "# X_train_selected, selector = feature_selection(X_train_resampled, Y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CODE FOR OULTLIER REMOVAL\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# from sklearn.covariance import EllipticEnvelope\n",
    "# from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Example data: Replace this with your actual data\n",
    "# # df = pd.read_csv('your_data.csv')\n",
    "# # Assuming df is your dataframe with features and 'outcome' column as the target\n",
    "# X = df.drop('outcome', axis=1)\n",
    "# Y = df['outcome']\n",
    "\n",
    "# # Split into train and test sets\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# # Apply one-hot encoding to categorical features\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# X_train_1 = encoder.fit_transform(X_train[['protocol_type']])\n",
    "# X_test_1 = encoder.transform(X_test[['protocol_type']])\n",
    "\n",
    "# X_train_2 = encoder.fit_transform(X_train[['service']])\n",
    "# X_test_2 = encoder.transform(X_test[['service']])\n",
    "\n",
    "# X_train_3 = encoder.fit_transform(X_train[['flag']])\n",
    "# X_test_3 = encoder.transform(X_test[['flag']])\n",
    "\n",
    "# # Concatenate encoded and numeric features\n",
    "# X_train_combined = np.concatenate([X_train_1, X_train_2, X_train_3, X_train.drop(['protocol_type', 'service', 'flag'], axis=1).values], axis=1)\n",
    "# X_test_combined = np.concatenate([X_test_1, X_test_2, X_test_3, X_test.drop(['protocol_type', 'service', 'flag'], axis=1).values], axis=1)\n",
    "\n",
    "# # Initialize RobustScaler and apply it to the training set\n",
    "# scaler = RobustScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "# X_test_scaled = scaler.transform(X_test_combined)\n",
    "\n",
    "# # Encode the target variable\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "# Y_test_encoded = label_encoder.transform(Y_test)\n",
    "\n",
    "# # --------------------------------\n",
    "# # Step 1: Detect Outliers (Isolation Forest, LOF, Elliptic Envelope)\n",
    "# # --------------------------------\n",
    "\n",
    "# # Isolation Forest\n",
    "# iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "# iso_forest.fit(X_train_scaled)\n",
    "# train_iso_pred = iso_forest.predict(X_train_scaled)\n",
    "# test_iso_pred = iso_forest.predict(X_test_scaled)\n",
    "\n",
    "# # Local Outlier Factor (LOF)\n",
    "# lof = LocalOutlierFactor(contamination=0.1)\n",
    "# train_lof_pred = lof.fit_predict(X_train_scaled)\n",
    "\n",
    "# # Elliptic Envelope\n",
    "# elliptic = EllipticEnvelope(contamination=0.1)\n",
    "# elliptic.fit(X_train_scaled)\n",
    "# train_elliptic_pred = elliptic.predict(X_train_scaled)\n",
    "# test_elliptic_pred = elliptic.predict(X_test_scaled)\n",
    "\n",
    "# # Convert outliers (-1) to 1, inliers (1) to 0 for binary classification\n",
    "# train_iso_pred[train_iso_pred == 1] = 0\n",
    "# train_iso_pred[train_iso_pred == -1] = 1\n",
    "# test_iso_pred[test_iso_pred == 1] = 0\n",
    "# test_iso_pred[test_iso_pred == -1] = 1\n",
    "\n",
    "# train_lof_pred[train_lof_pred == 1] = 0\n",
    "# train_lof_pred[train_lof_pred == -1] = 1\n",
    "\n",
    "# train_elliptic_pred[train_elliptic_pred == 1] = 0\n",
    "# train_elliptic_pred[train_elliptic_pred == -1] = 1\n",
    "# test_elliptic_pred[test_elliptic_pred == 1] = 0\n",
    "# test_elliptic_pred[test_elliptic_pred == -1] = 1\n",
    "\n",
    "# # --------------------------------\n",
    "# # Step 2: Remove Outliers Based on Isolation Forest\n",
    "# # --------------------------------\n",
    "\n",
    "# # Filter out outliers (where prediction is 0)\n",
    "# X_train_cleaned = X_train_scaled[train_iso_pred == 0]\n",
    "# Y_train_cleaned = Y_train_encoded[train_iso_pred == 0]\n",
    "\n",
    "# X_test_cleaned = X_test_scaled[test_iso_pred == 0]\n",
    "# Y_test_cleaned = Y_test_encoded[test_iso_pred == 0]\n",
    "\n",
    "# # --------------------------------\n",
    "# # Step 3: Train Classifier After Removing Outliers\n",
    "# # --------------------------------\n",
    "\n",
    "# # Example classifier: Logistic Regression\n",
    "# clf = LogisticRegression(random_state=42)\n",
    "# clf.fit(X_train_cleaned, Y_train_cleaned)\n",
    "\n",
    "# # Make predictions on cleaned test set\n",
    "# predictions = clf.predict(X_test_cleaned)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(Y_test_cleaned, predictions, zero_division=0))\n",
    "\n",
    "# accuracy = accuracy_score(Y_test_cleaned, predictions)\n",
    "# print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# # --------------------------------\n",
    "# # Optional: Try LOF and Elliptic Envelope to Clean Data\n",
    "# # --------------------------------\n",
    "# # For LOF or Elliptic Envelope, follow a similar approach:\n",
    "# # - Use the predictions from the models (train_lof_pred or train_elliptic_pred)\n",
    "# # - Filter out the outliers and retrain the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline function to execute all steps\n",
    "def complete_preprocessing_pipeline(df, target_column, categorical_columns):\n",
    "    # if resampled_data.csv exists dont apply smoteenn and mov to feature selection\n",
    "    if not os.path.exists('resampled_data.csv'):\n",
    "        print(\"Resampled data not found. Applying preprocessing steps...\")\n",
    "        create_directories()\n",
    "        X_train, X_test, Y_train, Y_test = load_and_split_data(df, target_column)\n",
    "        X_train_encoded, X_test_encoded, encoders = one_hot_encode(X_train, X_test, categorical_columns)\n",
    "        X_train_scaled, X_test_scaled, scaler = robust_scale(X_train_encoded, X_test_encoded)\n",
    "        # dump test data to csv\n",
    "        test_data = pd.DataFrame(X_test_scaled)\n",
    "        test_data['outcome'] = Y_test\n",
    "        test_data.to_csv('test_data.csv', index=False)\n",
    "        X_train_resampled, Y_train_resampled = apply_smoteenn(X_train_scaled, Y_train)\n",
    "    else:\n",
    "        print(\"Resampled data found. Skipping preprocessing steps...\")\n",
    "        resampled_data = pd.read_csv('resampled_data.csv')\n",
    "        X_train_resampled = resampled_data.drop('outcome', axis=1)\n",
    "        Y_train_resampled = resampled_data['outcome']\n",
    "        # apply label encoding to Y_train_resampled and then apply feature selection\n",
    "        label_encoder = LabelEncoder()\n",
    "        Y_train_resampled = label_encoder.fit_transform(Y_train_resampled)\n",
    "    X_train_selected, selector = feature_selection(X_train_resampled, Y_train_resampled)\n",
    "    # dump the selected features to csv\n",
    "    selected_data = pd.DataFrame(X_train_selected)\n",
    "    selected_data['outcome'] = Y_train_resampled\n",
    "    selected_data.to_csv('selected_data.csv', index=False)\n",
    "\n",
    "    # save_model_instance(encoders, 'encoders')\n",
    "    # save_model_instance(scaler, 'scaler')\n",
    "    # save_model_instance(selector, 'selector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled data found. Skipping preprocessing steps...\n",
      "Starting feature selection...\n",
      "Removed low correlation features: [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 35, 42, 44, 46]\n",
      "Removed highly correlated features: [2, 6, 8, 18, 19, 20, 21, 26, 27, 34]\n",
      "Selected 20 best features using mutual information.\n",
      "Saving model instance: selector...\n"
     ]
    }
   ],
   "source": [
    "complete_preprocessing_pipeline(df, target_column='outcome', categorical_columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoders, scaler and selector\n",
    "def load_model_instance(filename):\n",
    "    print(f\"Loading model instance: {filename}...\")\n",
    "    return np.load(f'models/{filename}.npy', allow_pickle=True).item()\n",
    "\n",
    "selector = load_model_instance('selector')\n",
    "\n",
    "# load from instances\n",
    "Train_data = pd.read_csv('resampled_data.csv')\n",
    "Test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# apply selector on train data \n",
    "\n",
    "X_train_resampled = Train_data.drop('outcome', axis=1)\n",
    "Y_train_resampled = Train_data['outcome']\n",
    "\n",
    "X_train_selected = selector.transform(X_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'accuracy_score' from 'sklearn.base' (/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# apply multinomial naive bayes on selected data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'accuracy_score' from 'sklearn.base' (/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py)"
     ]
    }
   ],
   "source": [
    "# apply multinomial naive bayes on selected data\n",
    "from sklearn.base import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# load selected data from selected_data.csv\n",
    "Train_data = pd.read_csv('selected_data.csv')\n",
    "Test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "X_train_selected = Train_data.drop('outcome', axis=1)\n",
    "Y_train_resampled = Train_data['outcome']\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_selected, Y_train_resampled)\n",
    "\n",
    "# apply selector on test data\n",
    "X_test = Test_data.drop('outcome', axis=1)\n",
    "Y_test = Test_data['outcome']\n",
    "\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# make predictions\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Classification Report:')\n",
    "print(classification_report(Y_test, predictions, zero_division=0))\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(clf, 'multinomialNBmodel.pkl')\n",
    "print(\"Model saved as model.pkl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply vanila perceptron model for multiclass classification\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train_selected, Y_train_resampled)\n",
    "\n",
    "# make predictions\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Classification Report:')\n",
    "print(classification_report(Y_test, predictions, zero_division=0))\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# save the model\n",
    "joblib.dump(clf, 'perceptronmodel.pkl')\n",
    "print(\"Model saved as perceptronmodel.pkl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply decision tree model for multiclasses\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_selected, Y_train_resampled)\n",
    "\n",
    "# make predictions\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Classification Report:')\n",
    "print(classification_report(Y_test, predictions, zero_division=0))\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the model\n",
    "joblib.dump(clf, 'decisiontreemodel.pkl')\n",
    "print(\"Model saved as decisiontreemodel.pkl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply softmax regression model for multiclasses\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(X_train_selected, Y_train_resampled)\n",
    "\n",
    "# make predictions\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Classification Report:')\n",
    "print(classification_report(Y_test, predictions, zero_division=0))\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the model\n",
    "joblib.dump(clf, 'softmaxregressionmodel.pkl')\n",
    "print(\"Model saved as softmaxregressionmodel.pkl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SVM model for multiclasses\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train_selected, Y_train_resampled)\n",
    "\n",
    "# make predictions\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Classification Report:')\n",
    "print(classification_report(Y_test, predictions, zero_division=0))\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# save the model\n",
    "joblib.dump(clf, 'svmmodel.pkl')\n",
    "print(\"Model saved as svmmodel.pkl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lazy predict for multiclass classification\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train_selected, X_test_selected, Y_train_resampled, Y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. feature report from claude also look for feature selection techniques from literature papers\n",
    "2. data vizualization\n",
    "3. why which preprocessing technique is used\n",
    "4. collect all results\n",
    "5. study results and plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
