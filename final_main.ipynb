{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Ensure the directories for saving data exist\n",
    "def create_directories():\n",
    "    if not os.path.exists('data/'):\n",
    "        os.makedirs('data/')\n",
    "        print(\"Created directory: data/\")\n",
    "    if not os.path.exists('models/'):\n",
    "        os.makedirs('models/')\n",
    "        print(\"Created directory: models/\")\n",
    "\n",
    "create_directories()  # Create the necessary directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "duration: continuous.\n",
    "protocol_type: symbolic.\n",
    "service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "def load_and_split_data(df, target_column='outcome', test_size=0.2, random_state=42):\n",
    "    print(\"Loading and splitting the dataset...\")\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    Y = df[target_column]  # Target\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state, stratify=Y)\n",
    "\n",
    "    # Save the split data\n",
    "    X_train.to_csv('data/X_train.csv', index=False)\n",
    "    X_test.to_csv('data/X_test.csv', index=False)\n",
    "    Y_train.to_csv('data/Y_train.csv', index=False)\n",
    "    Y_test.to_csv('data/Y_test.csv', index=False)\n",
    "\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "# Categorize the attack types into the 5 classes\n",
    "def categorize_attack_type(label):\n",
    "    if label in dos_attacks:\n",
    "        return 'DOS'\n",
    "    elif label in r2l_attacks:\n",
    "        return 'R2L'\n",
    "    elif label in u2r_attacks:\n",
    "        return 'U2R'\n",
    "    elif label in probe_attacks:\n",
    "        return 'probing'\n",
    "    else:\n",
    "        return 'normal'\n",
    "    \n",
    "\n",
    "# Example Usage\n",
    "df = pd.read_csv('data.csv')  # Replace with your actual dataset\n",
    "\n",
    "dos_attacks = ['smurf.', 'neptune.', 'back.', 'teardrop.', 'pod.', 'land.']\n",
    "r2l_attacks = ['warezclient.', 'guess_passwd.', 'imap.', 'warezmaster.', 'ftp_write.', 'phf.', 'spy.', 'multihop.']\n",
    "u2r_attacks = ['buffer_overflow.', 'loadmodule.', 'rootkit.', 'perl.']\n",
    "probe_attacks = ['satan.', 'ipsweep.', 'portsweep.', 'nmap.']\n",
    "df['outcome'] = df['outcome'].apply(categorize_attack_type)\n",
    "\n",
    "\n",
    "target_column = 'outcome'  # Define your target column\n",
    "X_train, X_test, Y_train, Y_test = load_and_split_data(df, target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical features\n",
    "def one_hot_encode(X_train, X_test, categorical_columns):\n",
    "    print(\"One-hot encoding categorical features...\")\n",
    "    encoders = {}\n",
    "    X_train_encoded_list = []\n",
    "    X_test_encoded_list = []\n",
    "    feature_names = []\n",
    "\n",
    "    # Loop through each categorical column and apply OneHotEncoder\n",
    "    for col in categorical_columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        # Fit and transform the training data\n",
    "        X_train_col_encoded = encoder.fit_transform(X_train[[col]])\n",
    "        X_train_encoded_list.append(X_train_col_encoded)\n",
    "        \n",
    "        # Transform the test data\n",
    "        X_test_col_encoded = encoder.transform(X_test[[col]])\n",
    "        X_test_encoded_list.append(X_test_col_encoded)\n",
    "\n",
    "        # Store the encoder for potential future use\n",
    "        encoders[col] = encoder\n",
    "        \n",
    "        # Get feature names and add to the feature names list\n",
    "        feature_names.extend(encoder.get_feature_names_out([col]))\n",
    "    \n",
    "    # Concatenate the encoded columns into a single matrix\n",
    "    X_train_encoded = np.concatenate(X_train_encoded_list, axis=1)\n",
    "    X_test_encoded = np.concatenate(X_test_encoded_list, axis=1)\n",
    "    \n",
    "    # Remove original categorical columns from X_train and X_test\n",
    "    X_train_remaining = X_train.drop(columns=categorical_columns)\n",
    "    X_test_remaining = X_test.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Concatenate the remaining columns with the one-hot encoded columns\n",
    "    X_train_final = np.concatenate([X_train_remaining.values, X_train_encoded], axis=1)\n",
    "    X_test_final = np.concatenate([X_test_remaining.values, X_test_encoded], axis=1)\n",
    "    \n",
    "    # Ensure the directory exists before saving the files\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "\n",
    "    # Save the encoded data\n",
    "    np.save('data/X_train_encoded.npy', X_train_final)\n",
    "    np.save('data/X_test_encoded.npy', X_test_final)\n",
    "\n",
    "    print(\"Categorical features encoded and original columns removed.\")\n",
    "    return X_train_final, X_test_final, encoders\n",
    "\n",
    "# Example usage with specified categorical columns\n",
    "categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "X_train_encoded, X_test_encoded, encoders = one_hot_encode(X_train, X_test, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Robust Scaling\n",
    "def robust_scale(X_train, X_test):\n",
    "    print(\"Applying robust scaling...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Save the scaled data\n",
    "    np.save('data/X_train_scaled.npy', X_train_scaled)\n",
    "    np.save('data/X_test_scaled.npy', X_test_scaled)\n",
    "\n",
    "    print(\"Data scaled using RobustScaler.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "X_train_scaled, X_test_scaled, scaler = robust_scale(X_train_encoded, X_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN  # Import SMOTEENN\n",
    "\n",
    "# Apply SMOTEENN resampling\n",
    "def apply_smoteenn(X_train, Y_train):\n",
    "    print(\"Applying SMOTEENN for resampling...\")\n",
    "    \n",
    "    # Initialize SMOTEENN with a suitable k_neighbors\n",
    "    smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "    \n",
    "    # Fit and resample the training data\n",
    "    X_train_resampled, Y_train_resampled = smote_enn.fit_resample(X_train, Y_train)\n",
    "\n",
    "    # Save resampled data\n",
    "    np.save('data/X_train_resampled.npy', X_train_resampled)\n",
    "    np.save('data/Y_train_resampled.npy', Y_train_resampled)\n",
    "\n",
    "    print(\"Data resampled using SMOTEENN.\")\n",
    "    print(f\"Resampled Y distribution: {Counter(Y_train_resampled)}\")\n",
    "    \n",
    "    return X_train_resampled, Y_train_resampled\n",
    "\n",
    "# Assuming X_train_scaled and Y_train are defined and preprocessed\n",
    "X_train_resampled, Y_train_resampled = apply_smoteenn(X_train_scaled, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def feature_selection(X_train, Y_train, variance_threshold=0.01, k_best=20, correlation_threshold=0.1):\n",
    "    print(\"Starting feature selection...\")\n",
    "\n",
    "    # Variance Threshold\n",
    "    vt = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_train_var = vt.fit_transform(X_train)\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = pd.DataFrame(X_train_var).corrwith(pd.Series(Y_train))\n",
    "    low_corr_features = correlation_matrix[correlation_matrix.abs() < correlation_threshold].index\n",
    "    X_train_var = np.delete(X_train_var, low_corr_features, axis=1)\n",
    "\n",
    "    print(f\"Removed low correlation features: {low_corr_features.tolist()}\")\n",
    "\n",
    "    # Keep one feature from highly correlated features\n",
    "    corr_matrix = pd.DataFrame(X_train_var).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "    # Retain only one feature from the highly correlated ones\n",
    "    X_train_var = np.delete(X_train_var, to_drop, axis=1)\n",
    "    print(f\"Removed highly correlated features: {to_drop}\")\n",
    "\n",
    "    # Mutual Information to select K best features\n",
    "    selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "    X_train_selected = selector.fit_transform(X_train_var, Y_train)\n",
    "\n",
    "    # Save the selected features\n",
    "    np.save('data/X_train_selected.npy', X_train_selected)\n",
    "\n",
    "    print(f\"Selected {k_best} best features using mutual information.\")\n",
    "    return X_train_selected, selector\n",
    "\n",
    "X_train_selected, selector = feature_selection(X_train_resampled, Y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoders and scalers\n",
    "def save_model_instance(instance, filename):\n",
    "    print(f\"Saving model instance: {filename}...\")\n",
    "    np.save(f'models/{filename}.npy', instance)\n",
    "\n",
    "# Save models\n",
    "save_model_instance(encoders, 'encoders')\n",
    "save_model_instance(scaler, 'scaler')\n",
    "save_model_instance(selector, 'selector')\n",
    "\n",
    "print(\"All models and instances saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline function to execute all steps\n",
    "def complete_pipeline(df, target_column, categorical_columns):\n",
    "    create_directories()\n",
    "    X_train, X_test, Y_train, Y_test = load_and_split_data(df, target_column)\n",
    "    X_train_encoded, X_test_encoded, encoders = one_hot_encode(X_train, X_test, categorical_columns)\n",
    "    X_train_scaled, X_test_scaled, scaler = robust_scale(X_train_encoded, X_test_encoded)\n",
    "    X_train_resampled, Y_train_resampled = apply_smoteenn(X_train_scaled, Y_train)\n",
    "    X_train_selected, selector = feature_selection(X_train_resampled, Y_train_resampled)\n",
    "    save_model_instance(encoders, 'encoders')\n",
    "    save_model_instance(scaler, 'scaler')\n",
    "    save_model_instance(selector, 'selector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pipeline(df, target_column='outcome', categorical_columns=categorical_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
